 # Manual Backpropagation Neural Network (NumPy Implementation)

A fully manual implementation of a two-layer neural network, including both forward and backward propagation, using only NumPy.  
This project was developed to build a deep, mathematical understanding of the backpropagation algorithm without relying on external machine learning frameworks.

---

## Overview

This repository contains:

- A clean Jupyter Notebook implementation of a neural network trained via manual backpropagation.
- Full derivation and application of gradient updates based on the chain rule.
- Visualization and tracking of loss during training.
- Intentionally minimalist code to maximize educational value.

The project demonstrates complete control over the training pipeline â€” from parameter initialization, to manual loss computation, to hand-derived gradient descent updates.

---

## Motivation

While modern libraries abstract away gradient computation, understanding backpropagation at a low level is essential for:

- Building intuition for how neural networks learn.
- Gaining the ability to debug, optimize, or design novel architectures.
- Preparing for research-level work where automatic differentiation may not be available.

This project was created to solidify core concepts critical for advanced machine learning, AI theory, and further academic research.

---


